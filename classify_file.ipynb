{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3da104de-2515-46fe-aebc-e5edfb665037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader, CSVLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492f4dec-64a4-49e7-bd9b-0e3fcbae85c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428507bb3d4c4db5944448a826a604ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "## Quantization config to reduce the size of the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                # bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type='nf4'\n",
    "                )\n",
    "\n",
    "## Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    # quantization_config=quantization_config,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "\n",
    "## Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8daf4b4-0109-4f22-86a5-fbd0d3560cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_paper_docs():\n",
    "    path = 'papers'\n",
    "    loader = PyPDFDirectoryLoader(path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a999c1-c58b-4aea-9118-b53a7f9e3802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "chunk_overlap = 100\n",
    "documents = get_paper_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf4a0fbd-d748-47f7-b88f-6b6a3a37c356",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "papers\\big data proposal.pdf\n",
      " ```python\n",
      "\n",
      "[\"Big Data Pipeline\", \"Real-Time Stock Market Analysis\", \"Yahoo Finance Data\", \"Apache Kafka\", \"Apache Spark\"]\n",
      "\n",
      "```\n",
      "papers\\Freyr Report.pdf\n",
      " ```python\n",
      "[\n",
      "    \"Serverless Computing\",\n",
      "    \"Resource Management\",\n",
      "    \"Freyr Serverless Resource Manager\",\n",
      "    \"Deep Reinforcement Learning\",\n",
      "    \"Performance Optimization\"\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.metadata['source'])\n",
    "    ## Split the document into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents([doc])\n",
    "    \n",
    "    ## Create prompt to pass to llm\n",
    "    query = f\"### INSTRUCTION: Create labels from the given text indicating the type of content. Generate 5 labels that most represent the file content. \\\n",
    "    Output the results as a python list. ### INPUT: {texts[0]}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    generation_args = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"return_full_text\": False,\n",
    "        # \"temperature\": 0.0,\n",
    "        \"do_sample\": False,\n",
    "    }\n",
    "\n",
    "    output = pipe(messages, **generation_args)\n",
    "    print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e94d6-8060-48d4-a856-fd4f47385969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07b3e6-4a99-4544-b865-c88a1eb7bc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
